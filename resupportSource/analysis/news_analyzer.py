# analysis/news_analyzer.py
import re
from datetime import datetime
from typing import List, Dict, Tuple
from collections import Counter
from utils.logger import setup_logger
from config.settings import get_config

class NewsAnalyzer:
    """æ–°é—»åˆ†æç±»"""
    
    def __init__(self):
        self.config = get_config()
        self.logger = setup_logger('NewsAnalyzer')
    
    def classify_news(self, title: str, content: str = "") -> str:
        """æ–°é—»åˆ†ç±»"""
        combined_text = (title + " " + content).lower()
        
        for category, keywords in self.config.CATEGORY_KEYWORDS.items():
            if any(keyword in combined_text for keyword in keywords):
                return category
        
        return 'å…¶ä»–'
    
    def extract_keywords(self, text: str, top_n: int = 10) -> List[Tuple[str, int]]:
        """æå–å…³é”®è¯"""
        # ä¸­æ–‡åˆ†è¯ï¼ˆç®€å•å®ç°ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨jiebaç­‰åˆ†è¯åº“ï¼‰
        words = re.findall(r'[\u4e00-\u9fa5]{2,6}', text)
        
        # è¿‡æ»¤åœç”¨è¯
        stopwords = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»–', 'å¥¹', 'å®ƒ'}
        filtered_words = [word for word in words if word not in stopwords and len(word) >= 2]
        
        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter(filtered_words)
        return word_counts.most_common(top_n)
    
    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        """æƒ…æ„Ÿåˆ†æï¼ˆç®€å•å®ç°ï¼‰"""
        # æ­£é¢è¯æ±‡
        positive_words = {'å¥½', 'ä¼˜ç§€', 'æˆåŠŸ', 'èƒœåˆ©', 'è¿›æ­¥', 'å‘å±•', 'å¢é•¿', 'æ”¹å–„', 'åˆ©å¥½', 'ç§¯æ', 'ä¹è§‚'}
        # è´Ÿé¢è¯æ±‡
        negative_words = {'å', 'å¤±è´¥', 'é—®é¢˜', 'å›°éš¾', 'ä¸‹é™', 'è¡°é€€', 'æ¶åŒ–', 'åˆ©ç©º', 'æ¶ˆæ', 'æ‚²è§‚', 'å±æœº'}
        
        words = set(re.findall(r'[\u4e00-\u9fa5]{2,4}', text))
        
        positive_count = len(words & positive_words)
        negative_count = len(words & negative_words)
        total_relevant = positive_count + negative_count
        
        if total_relevant == 0:
            return {'score': 0, 'sentiment': 'ä¸­æ€§'}
        
        score = (positive_count - negative_count) / total_relevant
        sentiment = 'æ­£é¢' if score > 0.1 else 'è´Ÿé¢' if score < -0.1 else 'ä¸­æ€§'
        
        return {
            'score': round(score, 3),
            'sentiment': sentiment,
            'positive_count': positive_count,
            'negative_count': negative_count
        }
    
    def analyze_hot_topics(self, news_list: List[Dict], top_n: int = 5) -> List[Dict]:
        """åˆ†æçƒ­ç‚¹è¯é¢˜"""
        all_titles = " ".join([news['title'] for news in news_list])
        keywords = self.extract_keywords(all_titles, top_n * 2)
        
        hot_topics = []
        for keyword, count in keywords[:top_n]:
            # æ‰¾åˆ°åŒ…å«è¯¥å…³é”®è¯çš„æ–°é—»
            related_news = [news for news in news_list if keyword in news['title']]
            
            topic = {
                'keyword': keyword,
                'count': count,
                'related_news_count': len(related_news),
                'platforms': list(set([news['platform'] for news in related_news])),
                'latest_news': related_news[0]['title'] if related_news else ""
            }
            hot_topics.append(topic)
        
        return hot_topics
    
    def generate_report(self, news_list: List[Dict]) -> Dict:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not news_list:
            return {
                'timestamp': datetime.now().isoformat(),
                'total_count': 0,
                'summary': 'æš‚æ— æ•°æ®'
            }
        
        # åˆ†ç±»ç»Ÿè®¡
        categorized = {}
        platform_stats = {}
        source_stats = {}
        sentiment_stats = {'æ­£é¢': 0, 'è´Ÿé¢': 0, 'ä¸­æ€§': 0}
        
        for news in news_list:
            # ç¡®ä¿åˆ†ç±»
            if 'category' not in news:
                news['category'] = self.classify_news(news['title'])
            
            # åˆ†ç±»ç»Ÿè®¡
            category = news['category']
            if category not in categorized:
                categorized[category] = []
            categorized[category].append(news)
            
            # å¹³å°ç»Ÿè®¡
            platform = news['platform']
            platform_stats[platform] = platform_stats.get(platform, 0) + 1
            
            # æ¥æºç»Ÿè®¡
            source = news.get('source', 'æœªçŸ¥')
            source_stats[source] = source_stats.get(source, 0) + 1
            
            # æƒ…æ„Ÿåˆ†æç»Ÿè®¡
            sentiment_result = self.analyze_sentiment(news['title'])
            sentiment_stats[sentiment_result['sentiment']] += 1
        
        # çƒ­ç‚¹åˆ†æ
        hot_topics = self.analyze_hot_topics(news_list)
        
        # æ—¶é—´åˆ†å¸ƒï¼ˆç®€å•æŒ‰æ—¥æœŸï¼‰
        date_stats = {}
        for news in news_list:
            date = news.get('publish_time', '')[:10]  # å–YYYY-MM-DD
            if date:
                date_stats[date] = date_stats.get(date, 0) + 1
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_count': len(news_list),
            'categorized_news': categorized,
            'platform_stats': platform_stats,
            'source_stats': source_stats,
            'sentiment_stats': sentiment_stats,
            'date_stats': date_stats,
            'hot_topics': hot_topics,
            'summary': self._generate_summary_text(categorized, platform_stats, hot_topics)
        }
        
        self.logger.info(f"ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼Œæ€»è®¡ {len(news_list)} æ¡æ–°é—»")
        return report
    
    def _generate_summary_text(self, categorized: Dict, platform_stats: Dict, 
                             hot_topics: List[Dict]) -> str:
        """ç”Ÿæˆæ–‡æœ¬ç®€æŠ¥"""
        summary = "ğŸ“Š æ–°é—»åˆ†æç®€æŠ¥\n"
        summary += "=" * 70 + "\n\n"
        
        # åŸºæœ¬ä¿¡æ¯
        total_count = sum(len(news_list) for news_list in categorized.values())
        summary += f"ğŸ“ˆ åŸºæœ¬ä¿¡æ¯:\n"
        summary += f"   æ€»è®¡æ–°é—»: {total_count} æ¡\n"
        summary += f"   å¹³å°åˆ†å¸ƒ: {', '.join([f'{k}({v})' for k, v in platform_stats.items()])}\n"
        summary += f"   åˆ†ç±»æ•°é‡: {len(categorized)} ç±»\n\n"
        
        # çƒ­ç‚¹è¯é¢˜
        if hot_topics:
            summary += "ğŸ”¥ çƒ­ç‚¹è¯é¢˜:\n"
            for i, topic in enumerate(hot_topics[:5], 1):
                summary += f"   {i}. {topic['keyword']} (å‡ºç°{topic['count']}æ¬¡)\n"
            summary += "\n"
        
        # åˆ†ç±»è¯¦æƒ…
        summary += "ğŸ·ï¸ åˆ†ç±»è¯¦æƒ…:\n"
        for category, news_list in sorted(categorized.items(), 
                                        key=lambda x: len(x[1]), reverse=True):
            summary += f"   {category}: {len(news_list)} æ¡\n"
        
        summary += "\n" + "=" * 70 + "\n"
        summary += f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        
        return summary
    
    def export_analysis_data(self, report: Dict, format_type: str = 'json') -> str:
        """å¯¼å‡ºåˆ†ææ•°æ®"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"news_analysis_{timestamp}.{format_type}"
        
        try:
            if format_type == 'json':
                import json
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(report, f, ensure_ascii=False, indent=2)
            
            elif format_type == 'txt':
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(report['summary'])
            
            else:
                self.logger.warning(f"ä¸æ”¯æŒçš„æ ¼å¼: {format_type}")
                return ""
            
            self.logger.info(f"åˆ†ææ•°æ®å·²å¯¼å‡º: {filename}")
            return filename
            
        except Exception as e:
            self.logger.error(f"å¯¼å‡ºåˆ†ææ•°æ®å¤±è´¥: {e}")
            return ""